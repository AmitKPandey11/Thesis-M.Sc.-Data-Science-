{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b43c448476403eb3e7ee01a04fdc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Extracting', max=3.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "from tqdm.notebook import tqdm\n",
    "data = pd.read_excel(r\"Desktop\\Book1.xlsx\")#Here give location to your file containing handler name with a header in file\n",
    "\n",
    "tmp = []\n",
    "na = []\n",
    "\n",
    "with tqdm(total=len(data), desc = 'Extracting') as pbar:\n",
    "    for name in data.iloc[:,0]: \n",
    "    \n",
    "        auth = tweepy.OAuthHandler('ENTER YOUR KEYS', 'ENTER YOUR KEYS') \n",
    "        auth.set_access_token('ENTER YOUR KEYS', 'ENTER YOUR KEYS') \n",
    "        api = tweepy.API(auth)\n",
    "\n",
    "        alltweets = []\n",
    "\n",
    "        new_tweets = api.user_timeline(screen_name = name,count=200)\n",
    "\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        while len(new_tweets) > 0:\n",
    "\n",
    "            new_tweets = api.user_timeline(screen_name = name,count=200,max_id=oldest)\n",
    "\n",
    "            alltweets.extend(new_tweets)\n",
    "\n",
    "            oldest = alltweets[-1].id - 1\n",
    "\n",
    "        outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "        for j in outtweets: \n",
    "            tmp.append(j)\n",
    "            na.append(name)\n",
    "        pbar.update(1)\n",
    "    \n",
    "df = pd.DataFrame({'Tweets':tmp, 'Handler':na})\n",
    "df.to_excel(r\"Desktop\\Twitter Output.xlsx\") # excel file will be export on desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating table scraping code Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032fe5844d3347b49e8bbc46aafb0fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Scraping', max=3.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from selenium import webdriver\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "data = pd.read_excel(r'Desktop\\Book2.xlsx')#Here give location to your file containing seller url's name with a header in file\n",
    "\n",
    "\n",
    "dfs=[]\n",
    "with tqdm(total=len(data), desc = 'Scraping') as pbar:\n",
    "    with webdriver.Chrome(r\"C:\\chromedriver_win32\\chromedriver.exe\") as driver:\n",
    "            for url in data.iloc[:,0]:\n",
    "                driver.get(url)\n",
    "                sleep(3)\n",
    "                soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "                d = []\n",
    "                positive=[]\n",
    "                neutral = []\n",
    "                negative=[]\n",
    "                count = []\n",
    "\n",
    "                for i in soup.findAll('td', class_ = 'a-text-right'):\n",
    "                    d.append(i.text.strip())\n",
    "                for i in d[:4]:\n",
    "                    positive.append(i)\n",
    "                for i in d[4:8]:\n",
    "                    neutral.append(i)\n",
    "                for i in d[8:12]:\n",
    "                    negative.append(i)\n",
    "                for i in d[12:]:\n",
    "                    count.append(i)\n",
    "\n",
    "                e = pd.DataFrame({'Positive':positive, 'Neutral':neutral, 'Negative':negative, 'Count':count})\n",
    "                e = e.T\n",
    "                df = e.set_axis(['30 days', '90 days', '12 months', 'Lifetime'], axis=1, inplace=False)\n",
    "                df['Seller URL'] = driver.current_url\n",
    "                dfs.append(df)\n",
    "                pbar.update(1)\n",
    "df_final = dfs[0]\n",
    "for i in range(len(dfs)-1):\n",
    "    df_final = pd.concat([df_final, dfs[i+1]])\n",
    "df_final\n",
    "df_final.to_excel(r'Desktop\\Amazon Output.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
